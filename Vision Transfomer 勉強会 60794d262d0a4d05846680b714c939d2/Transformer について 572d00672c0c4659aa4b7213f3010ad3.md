# Transformer について

## 論文　2017

[Attention Is All You Need](https://arxiv.org/abs/1706.03762)

## 自然言語処理モデルとして提案

Transformerは自然言語処理モデルとして定案された。

既存手法ではRNN, LSTM(Long Short Term Memory), GRU (Gated Recurrent Unit)がある

## 既存手法との違い

**RNN：**前時系列を逐次的に処理するため並列できず計算効率が悪い

![https://aismiley.co.jp/wp-content/uploads/2021/11/rnn-method-1024x269.png](https://aismiley.co.jp/wp-content/uploads/2021/11/rnn-method-1024x269.png)

**CNN：[並列して計算できるが、](https://tkengo.github.io/assets/img/understanding-convolutional-neural-networks-for-nlp/stride.png)**離れた位置のトーク同士の関係を捉えることが難しい

下：入力トークン、上：フィルター

![https://tkengo.github.io/assets/img/understanding-convolutional-neural-networks-for-nlp/stride.png](https://tkengo.github.io/assets/img/understanding-convolutional-neural-networks-for-nlp/stride.png)

**Transfromer：**離れたトークン同士の関係を捉えつつ、計算を並列化ができる。

→RNNやCNNを用いずSelf-Attentionという機構を活用

→Encoder-Decoder方の構造を採用

→RNNやCNNを用いるより精度が高い

![http://www.matsuoka-peoffice.com/wp-content/uploads/2020/12/transformer.jpg](http://www.matsuoka-peoffice.com/wp-content/uploads/2020/12/transformer.jpg)

機械翻訳ではEncoderで入力文章を特徴量化し、Decoderがそれ用いて目的の言語を生成する。

## Transformerベースの事前学習言語モデル

事前学習言語モデル：自然言語処理タスクのデータセットで学習を行って推論するモデル

→ファインチューニングによる転移学習で精度改善可能

### BERT(Bidirectional Encoder Representations from Transformers)

- google検索

[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

### GPT(Generativ Pre-trained Transformer)

[Generative Pre-Trained Transformer for Design Concept Generation: An Exploration](https://arxiv.org/abs/2111.08489)

## Transformerの弱点

入力の系列長が長くなると計算コストが高くなる

BERTでは最大512トークンに制限

→改善研究が行われている 

# オープンソースライブラリ

HugginFace

[Models - Hugging Face](https://huggingface.co/models?sort=downloads&search=BERT)